{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression.\n",
        "Simple Linear Regression (SLR) is a statistical method used to study the relationship between two variables ‚Äî\n",
        "one independent variable (X) and one dependent variable (Y).\n",
        "\n",
        "It helps to predict the value of Y (output) based on the value of X (input) by fitting a straight line through the data points.\n",
        "This line is known as the regression line and is represented by the equation:\n",
        " Y = mx +c\n",
        "\n",
        "Where:\n",
        "\n",
        "* Y ‚Üí Dependent variable (predicted output)\n",
        "\n",
        "* X ‚Üí Independent variable (input feature)\n",
        "\n",
        "* m ‚Üí Slope or coefficient (represents rate of change of Y with respect to X)\n",
        "\n",
        "* c ‚Üí Intercept (the value of Y when X = 0)\n",
        "\n",
        "Example:\n",
        "Predicting a student‚Äôs exam score (Y) based on the number of study hours (X)."
      ],
      "metadata": {
        "id": "nqcVikd4lJr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression (SLR) is based on certain key statistical assumptions that must be satisfied for the model to provide reliable results.\n",
        "These assumptions ensure that the estimated relationship between the variables is valid and unbiased.\n",
        "\n",
        "1. Linearity\n",
        "\n",
        "* The relationship between the independent variable (X) and the dependent variable (Y) must be linear.\n",
        "\n",
        "* This means that the change in Y is directly proportional to the change in X.\n",
        "Example: If study hours increase, exam scores increase proportionally.\n",
        "\n",
        "2Ô∏è. Independence of Errors\n",
        "\n",
        "* The residuals (errors) ‚Äî the difference between predicted and actual Y values ‚Äî should be independent of each other.\n",
        "\n",
        "* No correlation should exist between successive residuals.\n",
        " Example: In time-series data, this means there should be no autocorrelation.\n",
        "3Ô∏è. Homoscedasticity\n",
        "\n",
        "* The variance of errors should remain constant across all levels of the independent variable.\n",
        "\n",
        "* If error variance changes (e.g., increases with X), it indicates heteroscedasticity ‚Äî which violates this assumption.\n",
        "\n",
        "4Ô∏è. Normality of Errors\n",
        "\n",
        "* The residuals (errors) should be normally distributed around the regression line.\n",
        "\n",
        "* This assumption is crucial for making accurate statistical inferences (like confidence intervals and hypothesis testing).\n",
        "\n",
        "5Ô∏è. No Outliers or Extreme Values\n",
        "\n",
        "* There should be no extreme outliers that heavily influence the regression line.\n",
        "\n",
        "* Outliers can distort the slope (m) and intercept (c), leading to misleading results."
      ],
      "metadata": {
        "id": "_ADVcP9mmSlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "In the equation of Simple Linear Regression,\n",
        "\n",
        "Y=mX+c\n",
        "\n",
        "the coefficient m is known as the slope or regression coefficient.\n",
        "It represents the rate of change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n",
        "\n",
        "Interpretation of m:\n",
        "\n",
        "* m shows how much Y changes when X increases by 1 unit, while keeping other factors constant.\n",
        "\n",
        "* It defines the direction and strength of the relationship between X and Y.\n",
        "\n",
        "If m > 0:\n",
        "\n",
        "* There is a positive relationship between X and Y.\n",
        "* As X increases, Y also increases.\n",
        "Example: As study hours increase, exam scores increase.\n",
        "\n",
        "If m < 0:\n",
        "* There is a negative relationship between X and Y.\n",
        "\n",
        "* As X increases, Y decreases.\n",
        "Example: As the number of absences increases, exam scores decrease.\n",
        "\n",
        "Example Calculation:\n",
        "\n",
        "Suppose the regression equation is:\n",
        "\n",
        "Y=5X+20\n",
        "\n",
        "Here, m = 5\n",
        "‚Üí For every additional unit increase in X, Y increases by 5 units."
      ],
      "metadata": {
        "id": "eeM-sJ6jnSXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "In the Simple Linear Regression equation:\n",
        "Y=mX+c\n",
        "\n",
        "the term c is known as the intercept or constant term.\n",
        "It represents the predicted value of Y when the independent variable (X) is equal to 0.\n",
        "\n",
        "Meaning of Intercept (c):\n",
        "\n",
        "* The intercept is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "* It indicates the baseline value of the dependent variable before any effect of X is considered.\n",
        "\n",
        "Mathematical Interpretation:\n",
        "\n",
        "When X = 0,\n",
        "Y = c\n",
        "\n",
        "So, c gives the expected value of Y when there is no influence from the independent variable.\n",
        "\n",
        "Example:\n",
        "If the regression equation is:\n",
        "Y=5X+20\n",
        "Then:\n",
        "c = 20\n",
        "\n",
        "This means when X = 0, the predicted value of Y is 20.\n",
        " Example Interpretation:\n",
        "If X = ‚Äúhours studied‚Äù and Y = ‚Äúexam score,‚Äù then even if a student studies 0 hours, the model predicts a baseline score of 20 marks (perhaps due to prior knowledge or guessing).\n",
        "\n",
        "Graphical Understanding:\n",
        "\n",
        "* The intercept is the point where the regression line touches the Y-axis.\n",
        "\n",
        "* It helps in defining the starting point of the line before X starts affecting Y."
      ],
      "metadata": {
        "id": "VDGoOehin7Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do we calculate the slope (m) in Simple Linear Regression?\n",
        "\n",
        "In Simple Linear Regression, the slope (m) represents how much the dependent variable (Y) changes for every one-unit increase in the independent variable (X).\n",
        "It is calculated mathematically using the formula derived from the Least Squares Method.\n",
        "Formula for the Slope (m):\n",
        "m = Œ£[(Xi - XÃÑ)(Yi - »≤)] / Œ£[(Xi - XÃÑ)¬≤]\n",
        "\n",
        "Where:\n",
        "\n",
        "Xi = individual value of X\n",
        "\n",
        "Yi = individual value of Y\n",
        "\n",
        "XÃÑ = mean of X values\n",
        "\n",
        "»≤ = mean of Y values\n",
        "\n",
        "Steps to Calculate:\n",
        "\n",
        "1. Find the mean of X and Y.\n",
        "XÃÑ = Œ£X / n\n",
        "»≤ = Œ£Y / n\n",
        "\n",
        "2. Subtract the mean from each X and Y to get deviations.\n",
        "(Xi - XÃÑ) and (Yi - »≤)\n",
        "\n",
        "3. Multiply these deviations and sum them up.\n",
        "Œ£[(Xi - XÃÑ)(Yi - »≤)]\n",
        "\n",
        "4. Divide by the sum of squared deviations of X.\n",
        "Œ£[(Xi - XÃÑ)¬≤]\n",
        "\n",
        "Example:\n",
        "\n",
        "Dataset:\n",
        "X = [1, 2, 3, 4, 5]\n",
        "Y = [2, 4, 5, 4, 5]\n",
        "\n",
        "Step 1:\n",
        "XÃÑ = 3\n",
        "»≤ = 4\n",
        "\n",
        "Step 2:\n",
        "m = [(1-3)(2-4) + (2-3)(4-4) + (3-3)(5-4) + (4-3)(4-4) + (5-3)(5-4)] / [(1-3)¬≤ + (2-3)¬≤ + (3-3)¬≤ + (4-3)¬≤ + (5-3)¬≤]\n",
        "\n",
        "m = (4 + 0 + 0 + 0 + 2) / 10 = 0.6\n",
        "\n",
        "So, m = 0.6\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "If m = 0.6, it means that for every 1 unit increase in X, the predicted value of Y increases by 0.6 units."
      ],
      "metadata": {
        "id": "F_5zyu5Gov2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Answer:The least squares method is a mathematical technique used in Simple Linear Regression to find the best-fitting line through a set of data points.\n",
        "Its main goal is to minimize the sum of squared differences between the actual (observed) values and the predicted values.\n",
        "\n",
        "Purpose:\n",
        "\n",
        "The purpose of the least squares method is to:\n",
        "\n",
        "1. Find the values of slope (m) and intercept (c) that minimize the total prediction error.\n",
        "\n",
        "2. Ensure that the fitted regression line represents the relationship between X and Y as accurately as possible.\n",
        "\n",
        "3. Reduce the effect of random variations or noise in the data.\n",
        "\n",
        "How It Works:\n",
        "\n",
        "For each data point, calculate the error (residual) as:\n",
        "\n",
        "error = actual value - predicted value\n",
        "error = Yi - (mXi + c)\n",
        "\n",
        "Then, square these errors and sum them up:\n",
        "\n",
        "SSE (Sum of Squared Errors) = Œ£(Yi - (mXi + c))¬≤\n",
        "\n",
        "The least squares method finds the values of m and c that make SSE as small as possible.\n",
        "Example:\n",
        "\n",
        "If the observed points are (1,2), (2,3), (3,5),\n",
        "the least squares method adjusts m and c in the line Y = mX + c so that\n",
        "the squared distance between the line and the actual points is minimized.\n",
        "\n",
        "Why Use It:\n",
        "\n",
        "* Minimizes total error in predictions\n",
        "\n",
        "* Provides the most accurate linear relationship between X and Y\n",
        "\n",
        "* Makes statistical estimation efficient and unbiased"
      ],
      "metadata": {
        "id": "bTTr3PkUp-wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The coefficient of determination (R¬≤) is a statistical measure that shows how well the independent variable (X) explains the variation in the dependent variable (Y).\n",
        "It tells us how much of the total variation in Y is explained by the regression model.Formula for R¬≤:\n",
        "\n",
        "R¬≤ = 1 - (SSR / SST)\n",
        "\n",
        "Where:\n",
        "\n",
        "* SSR = Sum of Squared Residuals = Œ£(Yi - ≈∂i)¬≤\n",
        "\n",
        "* SST = Total Sum of Squares = Œ£(Yi - »≤)¬≤\n",
        "\n",
        "* Yi = actual value\n",
        "\n",
        "* ≈∂i = predicted value\n",
        "\n",
        "* »≤ = mean of Y\n",
        "\n",
        "Interpretation of R¬≤:\n",
        "\n",
        "* R¬≤ = 0: The model does not explain any variation in Y.\n",
        "\n",
        "* R¬≤ = 1: The model perfectly explains all variation in Y.\n",
        "\n",
        "* 0 < R¬≤ < 1: The model explains part of the variation ‚Äî the higher the R¬≤, the better the model fits the data.\n",
        "\n",
        "Example:\n",
        "\n",
        "If R¬≤ = 0.85, it means 85% of the variation in the dependent variable (Y) is explained by the independent variable (X),\n",
        "and the remaining 15% is due to other factors or random noise.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "1. R¬≤ measures goodness of fit ‚Äî how well the regression line fits the data.\n",
        "\n",
        "2. A higher R¬≤ means a better fit, but an R¬≤ that is too high may indicate overfitting.\n",
        "\n",
        "3. R¬≤ cannot decrease when you add more variables (in multiple regression)."
      ],
      "metadata": {
        "id": "kHbcmtNnqidN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is Multiple Linear Regression?\n",
        "\n",
        "Answer:\n",
        "Multiple Linear Regression (MLR) is an extension of Simple Linear Regression that uses two or more independent variables (X‚ÇÅ, X‚ÇÇ, X‚ÇÉ, ...) to predict the value of a single dependent variable (Y).\n",
        "It helps to understand the relationship between one outcome and several predictors simultaneously.\n",
        "\n",
        "Equation of Multiple Linear Regression:\n",
        "\n",
        "Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + b‚ÇÉX‚ÇÉ + ... + bnXn + Œµ\n",
        "\n",
        "Where:\n",
        "\n",
        "* Y = Dependent variable (target)\n",
        "* b‚ÇÄ = Intercept (value of Y when all X‚Äôs are 0)\n",
        "* b‚ÇÅ, b‚ÇÇ, ..., bn = Coefficients (effect of each independent variable)\n",
        "* X‚ÇÅ, X‚ÇÇ, ..., Xn = Independent variables (predictors)\n",
        "* Œµ = Error term (difference between actual and predicted values)\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose we want to predict house price (Y) based on:\n",
        "* X‚ÇÅ: House size (in sq. ft.)\n",
        "* X‚ÇÇ: Number of bedrooms\n",
        "* X‚ÇÉ: Location rating\n",
        "\n",
        "Then the model would be:\n",
        "\n",
        "Price = b‚ÇÄ + b‚ÇÅ*(Size) + b‚ÇÇ*(Bedrooms) + b‚ÇÉ*(Location) + Œµ\n",
        "\n",
        "Purpose:\n",
        "\n",
        "* To predict an outcome using multiple influencing factors.\n",
        "* To understand the individual impact of each independent variable on the dependent variable.\n",
        "\n",
        "Key Benefits:\n",
        "\n",
        "1. Improves prediction accuracy by considering multiple features.\n",
        "\n",
        "2. Helps identify which variables have significant effects on the outcome.\n",
        "\n",
        "3. Useful in business, finance, and data science for multivariable analysis."
      ],
      "metadata": {
        "id": "Zs6Mhmilq9vM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The main difference between Simple Linear Regression (SLR) and Multiple Linear Regression (MLR) lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "1Ô∏è. Simple Linear Regression (SLR):\n",
        "\n",
        "* Uses only one independent variable (X) to predict a dependent variable (Y).\n",
        "* The relationship is represented by a straight line.\n",
        "\n",
        "Equation:\n",
        "Y = mX + c\n",
        "\n",
        "Example:\n",
        "Predicting a student‚Äôs exam score (Y) based on hours studied (X).\n",
        "\n",
        "2Ô∏è. Multiple Linear Regression (MLR):\n",
        "\n",
        "* Uses two or more independent variables (X‚ÇÅ, X‚ÇÇ, X‚ÇÉ, ‚Ä¶) to predict a single dependent variable (Y).\n",
        "* Represents a relationship in a multidimensional plane.\n",
        "\n",
        "Equation:\n",
        "Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + b‚ÇÉX‚ÇÉ + ... + Œµ\n",
        "\n",
        "Example:\n",
        "Predicting house price (Y) based on area (X‚ÇÅ), number of rooms (X‚ÇÇ), and location (X‚ÇÉ).\n"
      ],
      "metadata": {
        "id": "PoGb0uYJroII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Multiple Linear Regression (MLR) is based on several key statistical assumptions that must be met for the model to be valid and reliable.\n",
        "If these assumptions are violated, the predictions and coefficient estimates may become biased or inaccurate.\n",
        "\n",
        "1Ô∏è. Linearity\n",
        "\n",
        "* The relationship between the dependent variable (Y) and each independent variable (X‚ÇÅ, X‚ÇÇ, ‚Ä¶) should be linear.\n",
        "\n",
        "* This means a unit change in X should cause a proportional change in Y.\n",
        "\n",
        "2Ô∏è. Independence of Errors\n",
        "\n",
        "* The residuals (errors) should be independent of each other.\n",
        "\n",
        "* No correlation should exist between the errors of one observation and another.\n",
        "(This is especially important in time-series data.)\n",
        "\n",
        "3Ô∏è. Homoscedasticity\n",
        "\n",
        "* The variance of residuals should be constant across all levels of independent variables.\n",
        "\n",
        "* If error variance changes (called heteroscedasticity), it can distort model performance.\n",
        "\n",
        "4Ô∏è. Normality of Residuals\n",
        "\n",
        "* The residuals (errors) should be normally distributed.\n",
        "\n",
        "* This is important for hypothesis testing and confidence interval estimation.\n",
        "\n",
        "5Ô∏è. No Multicollinearity\n",
        "\n",
        "* The independent variables should not be highly correlated with each other.\n",
        "\n",
        "* High correlation (multicollinearity) makes it difficult to determine the effect of each variable on Y.\n",
        "(Variance Inflation Factor ‚Äî VIF ‚Äî is often used to detect this problem.)\n",
        "\n",
        "6Ô∏è. No Autocorrelation\n",
        "\n",
        "* Especially in time-series data, residuals should not show a systematic pattern over time.\n",
        "\n",
        "* Autocorrelation violates the assumption of independent errors."
      ],
      "metadata": {
        "id": "XX35S-MZr9Vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 11: What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Heteroscedasticity occurs when the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variables.\n",
        "In simpler terms, it means that the spread of errors increases or decreases as the value of X changes.\n",
        "\n",
        "1Ô∏è. Example:\n",
        "\n",
        "If you are predicting income (Y) based on years of education (X),\n",
        "the error (difference between actual and predicted income) might be smaller for low-income groups and larger for high-income groups.\n",
        "This unequal spread of errors indicates heteroscedasticity.\n",
        "\n",
        "2Ô∏è. Visual Identification:\n",
        "\n",
        "* In a scatter plot of residuals vs. predicted values:\n",
        "* Homoscedasticity: Points are evenly spread (random pattern).\n",
        "\n",
        "* Heteroscedasticity: Points form a funnel or cone shape, showing non-constant variance.\n",
        "\n",
        "3Ô∏è. Causes of Heteroscedasticity:\n",
        "\n",
        "* Presence of outliers in the dataset.\n",
        "\n",
        "* Skewed or non-normal data distribution.\n",
        "\n",
        "* Omission of important variables.\n",
        "\n",
        "* Using incorrect model form (missing nonlinear relationships).\n",
        "\n",
        "4Ô∏è. Effects on Regression Results:\n",
        "\n",
        "* Coefficients (b‚ÇÅ, b‚ÇÇ, ‚Ä¶) remain unbiased, but\n",
        "\n",
        "* Standard errors become unreliable, leading to:\n",
        "\n",
        "* Incorrect t-tests and p-values\n",
        "\n",
        "* Misleading confidence intervals\n",
        "\n",
        "* Reduced statistical efficiency\n",
        "\n",
        "So, heteroscedasticity affects the reliability of hypothesis testing, not the direction of coefficients.\n",
        "\n",
        "5Ô∏è. How to Detect Heteroscedasticity:\n",
        "\n",
        "* Plot residuals vs. fitted values.\n",
        "\n",
        "* Use statistical tests like:\n",
        "\n",
        "* Breusch‚ÄìPagan Test\n",
        "\n",
        "* White Test\n",
        "\n",
        "6Ô∏è. How to Fix It:\n",
        "\n",
        "* Transform variables (e.g., use log(Y) or sqrt(Y)).\n",
        "\n",
        "* Use Weighted Least Squares (WLS) instead of Ordinary Least Squares.\n",
        "\n",
        "* Add missing variables or interaction terms."
      ],
      "metadata": {
        "id": "cpz0LOTSsnRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 12: How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other.\n",
        "This makes it difficult for the model to distinguish the individual effect of each variable on the dependent variable (Y).\n",
        "\n",
        "High multicollinearity leads to:\n",
        "\n",
        "Unstable regression coefficients\n",
        "\n",
        "Large standard errors\n",
        "\n",
        "Incorrect significance tests (p-values)\n",
        "\n",
        "1Ô∏è. Identifying Multicollinearity\n",
        "\n",
        "You can detect multicollinearity using the following methods:\n",
        "\n",
        "Correlation Matrix: High pairwise correlation between independent variables (greater than 0.8 or 0.9).\n",
        "\n",
        "Variance Inflation Factor (VIF):\n",
        "\n",
        "VIF = 1 ‚Üí No multicollinearity\n",
        "\n",
        "VIF between 1‚Äì5 ‚Üí Moderate multicollinearity\n",
        "\n",
        "VIF > 10 ‚Üí High multicollinearity (problematic)\n",
        "\n",
        "2Ô∏è. Ways to Improve the Model\n",
        "a) Remove Highly Correlated Variables\n",
        "\n",
        "Drop one of the correlated predictors that provides similar information.\n",
        "\n",
        "Example: If ‚ÄúHeight‚Äù and ‚ÄúArm Length‚Äù are highly correlated, keep only one.\n",
        "\n",
        "b) Combine Correlated Variables\n",
        "\n",
        "Create a new variable by combining correlated features (e.g., average or ratio).\n",
        "\n",
        "Example: Combine ‚ÄúTotal Marks‚Äù = ‚ÄúMath Marks + Science Marks‚Äù.\n",
        "\n",
        "c) Use Dimensionality Reduction Techniques\n",
        "\n",
        "Apply Principal Component Analysis (PCA) to reduce correlated variables into fewer independent components.\n",
        "\n",
        "d) Regularization Techniques\n",
        "\n",
        "Use models like Ridge Regression or Lasso Regression that penalize large coefficients and help handle multicollinearity.\n",
        "\n",
        "e) Center or Standardize the Data\n",
        "\n",
        "Transform variables into standardized form (mean = 0, std = 1) to reduce numerical dependency among variables.\n",
        "\n",
        "3Ô∏è. Why Fixing Multicollinearity Matters\n",
        "\n",
        "It makes coefficient estimates more stable and interpretable.\n",
        "\n",
        "Improves prediction accuracy.\n",
        "\n",
        "Ensures correct p-values and confidence intervals."
      ],
      "metadata": {
        "id": "gZcb4MQctZcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 13: What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Categorical variables contain text or label values (like ‚ÄúMale/Female‚Äù or ‚ÄúCity names‚Äù) that cannot be directly used in regression models, since regression requires numeric input.\n",
        "Hence, we must convert categorical variables into numerical form using encoding techniques.\n",
        "\n",
        "1Ô∏è. One-Hot Encoding\n",
        "\n",
        "* Creates a new binary column (0 or 1) for each category in the variable.\n",
        "\n",
        "* Used when there is no natural order between categories (nominal data).\n",
        "\n",
        "Example:\n",
        "City ‚Üí [Delhi, Mumbai, Kolkata]\n",
        "| City_Delhi | City_Mumbai | City_Kolkata |\n",
        "| ---------- | ----------- | ------------ |\n",
        "| 1          | 0           | 0            |\n",
        "| 0          | 1           | 0            |\n",
        "| 0          | 0           | 1            |\n",
        "Note: One category is usually dropped to avoid the dummy variable trap.\n",
        "\n",
        "2Ô∏è. Label Encoding\n",
        "\n",
        "* Assigns a unique number to each category.\n",
        "\n",
        "* Used when categories have natural order (ordinal data).\n",
        "\n",
        "Example:\n",
        "Size ‚Üí [Small, Medium, Large]\n",
        "- becomes\n",
        "| Size |\n",
        "| ---- |\n",
        "| 1    |\n",
        "| 2    |\n",
        "| 3    |\n",
        "Limitation: Implies an artificial numeric relationship, so use carefully.\n",
        "\n",
        "3Ô∏è. Ordinal Encoding\n",
        "\n",
        "Similar to Label Encoding but used when there‚Äôs a true ranking or order between categories.\n",
        "\n",
        "Example: Education Level (High School < Bachelor < Master < PhD).\n",
        "\n",
        "4Ô∏è. Binary Encoding\n",
        "\n",
        "A hybrid of One-Hot and Label Encoding.\n",
        "\n",
        "Converts categories to binary (base 2) representation to reduce dimensionality.\n",
        "\n",
        "Example:\n",
        "If a category variable has 4 levels, it creates log‚ÇÇ(4) = 2 new columns.\n",
        "\n",
        "5Ô∏è. Target Encoding (Mean Encoding)\n",
        "\n",
        "Replaces each category with the mean value of the target variable for that category.\n",
        "\n",
        "Useful for high-cardinality categorical features.\n",
        "\n",
        "Must be used carefully to avoid overfitting.\n",
        "\n",
        "Example:\n",
        "If average income for ‚ÄúMale‚Äù = 50k and for ‚ÄúFemale‚Äù = 55k,\n",
        "then replace ‚ÄúGender‚Äù with [50, 55].\n"
      ],
      "metadata": {
        "id": "9GP3J8NhtssY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 14: What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In Multiple Linear Regression, interaction terms are used to capture the combined effect of two or more independent variables on the dependent variable (Y).\n",
        "They help identify whether the relationship between one predictor and the outcome depends on another variable.\n",
        "1Ô∏è. Meaning of Interaction Term\n",
        "\n",
        "An interaction term is created by multiplying two independent variables together.\n",
        "\n",
        "Example:\n",
        "If you have two predictors ‚Äî X‚ÇÅ (Experience) and X‚ÇÇ (Training Hours) ‚Äî\n",
        "the interaction term would be (X‚ÇÅ √ó X‚ÇÇ).\n",
        "\n",
        "Regression Equation:\n",
        "Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + b‚ÇÉ(X‚ÇÅ*X‚ÇÇ) + Œµ\n",
        "Here, b‚ÇÉ represents the interaction effect ‚Äî how the impact of one variable (X‚ÇÅ) on Y changes when the other variable (X‚ÇÇ) changes.\n",
        "\n",
        "2Ô∏è. Why Interaction Terms Are Important\n",
        "\n",
        "They allow the model to represent non-additive relationships between predictors.\n",
        "\n",
        "Show how one factor modifies the effect of another.\n",
        "\n",
        "Improve model accuracy when variables influence each other.\n",
        "\n",
        "3Ô∏è. Example:\n",
        "\n",
        "Suppose we want to predict employee performance (Y) using:\n",
        "\n",
        "X‚ÇÅ = Experience (years)\n",
        "\n",
        "X‚ÇÇ = Training Hours\n",
        "\n",
        "A model without interaction assumes that training has the same effect on all employees.\n",
        "But if training helps experienced employees more, the interaction term (Experience √ó Training) captures that combined effect.\n",
        "\n",
        "4Ô∏è. When to Use Interaction Terms\n",
        "\n",
        "When you suspect that two predictors together affect the outcome differently than when they act alone.\n",
        "\n",
        "Common in marketing, social sciences, and behavioral data.\n",
        "\n",
        "5Ô∏è. Interpreting the Interaction Term\n",
        "\n",
        "If b‚ÇÉ > 0 ‚Üí The effect of one variable increases when the other variable increases.\n",
        "\n",
        "If b‚ÇÉ < 0 ‚Üí The effect of one variable decreases when the other variable increases.\n"
      ],
      "metadata": {
        "id": "oqY0frliwVHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 15: How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The intercept represents the predicted value of the dependent variable (Y) when all independent variables (X‚Äôs) are equal to zero.\n",
        "However, its interpretation differs between Simple Linear Regression (SLR) and Multiple Linear Regression (MLR).\n",
        "\n",
        "1Ô∏è. In Simple Linear Regression (SLR):\n",
        "\n",
        "Equation:\n",
        "Y = mX + c\n",
        "\n",
        "Here,\n",
        "\n",
        "c = intercept = value of Y when X = 0.\n",
        "\n",
        "It represents the starting point of the regression line.\n",
        "\n",
        "Example:\n",
        "If Y = 5X + 20\n",
        "Then when X = 0, Y = 20 ‚Üí the predicted baseline value of Y.\n",
        "\n",
        " Interpretation:\n",
        "In a study of ‚Äúhours studied (X)‚Äù vs. ‚Äúexam score (Y),‚Äù\n",
        "the intercept (20) means that a student who studies 0 hours is expected to score 20 marks.\n",
        "\n",
        "2Ô∏è. In Multiple Linear Regression (MLR):\n",
        "\n",
        "Equation:\n",
        "Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + ... + bnXn + Œµ\n",
        "\n",
        "Here,\n",
        "\n",
        "b‚ÇÄ = intercept = predicted value of Y when all X‚ÇÅ, X‚ÇÇ, ..., Xn = 0.\n",
        "\n",
        "It represents the baseline level of Y after controlling for all variables.\n",
        "\n",
        "Example:\n",
        "If Y = 50 + 2X‚ÇÅ + 3X‚ÇÇ\n",
        "then when X‚ÇÅ = 0 and X‚ÇÇ = 0, predicted Y = 50.\n",
        "\n",
        " Interpretation:\n",
        "In a model predicting ‚ÄúSalary (Y)‚Äù based on ‚ÄúExperience (X‚ÇÅ)‚Äù and ‚ÄúEducation (X‚ÇÇ),‚Äù\n",
        "the intercept (50) means the predicted salary for someone with 0 experience and 0 education ‚Äî which may not always make real-world sense."
      ],
      "metadata": {
        "id": "nPu36whfws5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 16: What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In regression analysis, the slope (also called the regression coefficient) represents the rate of change in the dependent variable (Y) for every one-unit change in the independent variable (X), keeping all other variables constant.\n",
        "\n",
        "It shows the direction and strength of the relationship between the predictor and the outcome.\n",
        "\n",
        "1Ô∏è. Significance of the Slope:\n",
        "\n",
        "The slope tells how much Y changes when X increases by one unit.\n",
        "\n",
        "It helps to interpret the influence of each independent variable on the target variable.\n",
        "\n",
        "A positive slope means X and Y move in the same direction.\n",
        "\n",
        "A negative slope means X and Y move in opposite directions.\n",
        "\n",
        "2Ô∏è. Interpretation in Simple Linear Regression:\n",
        "\n",
        "Equation:\n",
        "Y = mX + c\n",
        "Here, m is the slope.\n",
        "\n",
        "Example:\n",
        "If Y = 5X + 20\n",
        "Then, for every 1 unit increase in X, Y increases by 5 units.\n",
        "\n",
        "Meaning:\n",
        "If X = ‚Äúhours studied‚Äù and Y = ‚Äúexam score,‚Äù\n",
        "then every extra hour of study increases the expected score by 5 marks.\n",
        "\n",
        "3Ô∏è. Interpretation in Multiple Linear Regression:\n",
        "\n",
        "Equation:\n",
        "Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + ... + Œµ\n",
        "\n",
        "Here, each b·µ¢ (slope coefficient) represents the change in Y for a one-unit change in X·µ¢, holding other variables constant.\n",
        "\n",
        "Example:\n",
        "If Salary = 20,000 + 3,000*(Experience) + 2,000*(EducationLevel)\n",
        "then:\n",
        "\n",
        "b‚ÇÅ = 3,000 ‚Üí Every extra year of experience increases salary by ‚Çπ3,000.\n",
        "\n",
        "b‚ÇÇ = 2,000 ‚Üí Each higher education level increases salary by ‚Çπ2,000.\n",
        "\n",
        "4Ô∏è. Sign of the Slope:\n",
        "Sign of Slope\tMeaning\n",
        "Positive (+)\tAs X increases, Y increases\n",
        "Negative (‚Äì)\tAs X increases, Y decreases\n",
        "Zero (0)\tNo relationship between X and Y\n",
        "5Ô∏è. Importance in Predictions:\n",
        "\n",
        "Determines the direction of prediction (increase or decrease in Y).\n",
        "\n",
        "Defines how sensitive Y is to changes in X.\n",
        "\n",
        "Helps in decision-making and trend forecasting in business, finance, and science."
      ],
      "metadata": {
        "id": "hcNBsyefw8Sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 17: How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The intercept (also called the constant term) in a regression model provides important context for understanding the relationship between the independent variable(s) and the dependent variable (Y).\n",
        "It represents the expected value of Y when all independent variables (X‚Äôs) are equal to zero.\n",
        "\n",
        "1Ô∏è. Meaning of the Intercept\n",
        "\n",
        "It shows the starting point or baseline value of the dependent variable before any effect from the independent variables occurs.\n",
        "\n",
        "In the regression equation:\n",
        "Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + ... + Œµ\n",
        "b‚ÇÄ (intercept) = predicted value of Y when all X‚Äôs = 0.\n",
        "\n",
        "2Ô∏è. Interpretation Example\n",
        "\n",
        "Example 1 (Simple Linear Regression):\n",
        "If Y = 4X + 10\n",
        "Then intercept = 10, meaning when X = 0, Y = 10.\n",
        " Interpretation:\n",
        "If X = ‚Äúhours studied‚Äù and Y = ‚Äúexam score,‚Äù a student who studies 0 hours is predicted to score 10 marks.\n",
        "\n",
        "Example 2 (Multiple Linear Regression):\n",
        "If Salary = 15,000 + 2,000*(Experience) + 3,000*(EducationLevel)\n",
        "Then the intercept = 15,000 ‚Üí this represents the baseline salary for a person with 0 years of experience and 0 education level.\n",
        "\n",
        "3Ô∏è. How It Provides Context\n",
        "\n",
        "It defines the reference point for the regression line or plane.\n",
        "\n",
        "It allows us to anchor predictions ‚Äî giving meaning to other coefficients (slopes).\n",
        "\n",
        "It helps interpret what the predicted outcome would be before any predictors are applied.\n",
        "\n",
        "In multiple regression, it adjusts the entire model vertically so that predictions fit the data properly.\n",
        "\n",
        "4Ô∏è. When the Intercept May Not Be Meaningful\n",
        "\n",
        "When ‚ÄúX = 0‚Äù has no real-world meaning (e.g., 0 years of education or 0 height).\n",
        "\n",
        "In such cases, the intercept still serves as a mathematical baseline but has no practical interpretation."
      ],
      "metadata": {
        "id": "RsvKc7dgxVTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 18: What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The coefficient of determination (R¬≤) measures how well a regression model explains the variation in the dependent variable (Y).\n",
        "While R¬≤ is useful, it has several limitations and should not be used alone to judge model performance.\n",
        "\n",
        "1Ô∏è. R¬≤ Does Not Indicate Causation\n",
        "\n",
        "A high R¬≤ value means variables are correlated, but it does not prove that X causes Y.\n",
        "\n",
        "Correlation does not imply causation.\n",
        "\n",
        "Example:\n",
        "There might be a high R¬≤ between ‚Äúice cream sales‚Äù and ‚Äúdrowning incidents,‚Äù but one does not cause the other ‚Äî both depend on temperature.\n",
        "\n",
        "2Ô∏è. R¬≤ Always Increases When You Add More Variables\n",
        "\n",
        "In Multiple Linear Regression, adding more independent variables always increases or keeps R¬≤ constant, even if the new variables are irrelevant.\n",
        "\n",
        "This can make the model look better than it actually is.\n",
        "\n",
        " That‚Äôs why Adjusted R¬≤ is preferred ‚Äî it penalizes unnecessary variables.\n",
        "\n",
        "3Ô∏è. R¬≤ Doesn‚Äôt Measure Model Accuracy\n",
        "\n",
        "A model can have a high R¬≤ but still make poor predictions on new data due to overfitting.\n",
        "\n",
        "R¬≤ only measures how well the model fits the training data.\n",
        "\n",
        "4Ô∏è. R¬≤ Doesn‚Äôt Indicate Model Bias\n",
        "\n",
        "A high R¬≤ doesn‚Äôt mean the model is correct.\n",
        "\n",
        "It cannot detect issues like outliers, multicollinearity, or violations of regression assumptions.\n",
        "\n",
        "5Ô∏è. R¬≤ is Meaningless for Non-Linear Models\n",
        "\n",
        "R¬≤ is mainly designed for linear relationships.\n",
        "\n",
        "In non-linear regression models, R¬≤ may give misleading results.\n",
        "\n",
        "6Ô∏è. R¬≤ Alone Ignores the Scale of Errors\n",
        "\n",
        "It doesn‚Äôt tell you the magnitude of errors (how far predictions are from actual values).\n",
        "\n",
        "Metrics like RMSE, MAE, or MAPE are needed to measure true prediction accuracy."
      ],
      "metadata": {
        "id": "EJRFbfBSyAXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 19: How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In regression analysis, each coefficient (slope) has an associated standard error (SE), which measures the accuracy and stability of that coefficient‚Äôs estimate.\n",
        "A large standard error indicates that the estimated coefficient is less reliable and may vary greatly if the model is fitted to a different sample of data.\n",
        "\n",
        "1Ô∏è. Meaning of Standard Error (SE)\n",
        "\n",
        "It represents the average distance between the estimated coefficient and its true (actual) value.\n",
        "\n",
        "A small SE ‚Üí coefficient is precisely estimated.\n",
        "\n",
        "A large SE ‚Üí coefficient estimate is uncertain and unstable.\n",
        "\n",
        "2Ô∏è. Interpretation of a Large Standard Error\n",
        "\n",
        "If a regression coefficient has a large SE:\n",
        "\n",
        "The model is not confident about that variable‚Äôs estimated effect.\n",
        "\n",
        "The corresponding t-value (coefficient √∑ SE) becomes smaller, leading to high p-values.\n",
        "\n",
        "This means the variable may not be statistically significant in explaining the dependent variable.\n",
        "\n",
        "3Ô∏è. Causes of Large Standard Errors\n",
        "\n",
        "Multicollinearity: Independent variables are highly correlated.\n",
        "\n",
        "Small Sample Size: Insufficient data points to estimate coefficients accurately.\n",
        "\n",
        "High Variability in Data: Inconsistent or noisy data.\n",
        "\n",
        "Incorrect Model Specification: Missing important variables or using irrelevant ones.\n",
        "\n",
        "4Ô∏è. Example\n",
        "\n",
        "Suppose you‚Äôre predicting salary using experience and education:\n",
        "\n",
        "Variable\tCoefficient\tStd. Error\tt-value\n",
        "Experience\t2.5\t0.4\t6.25\n",
        "Education\t1.2\t2.1\t0.57\n",
        "\n",
        "Here, Education has a large SE (2.1) compared to Experience (0.4),\n",
        "which means the effect of education on salary is uncertain and possibly not statistically significant.\n",
        "\n",
        "5Ô∏è. How to Fix Large Standard Errors\n",
        "\n",
        "Remove or combine correlated predictors (reduce multicollinearity).\n",
        "\n",
        "Increase sample size.\n",
        "\n",
        "Simplify the model by removing irrelevant variables.\n",
        "\n",
        "Standardize variables to reduce scale differences."
      ],
      "metadata": {
        "id": "M7KYKRePyU6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 20: What is the difference between Adjusted R¬≤ and R¬≤, and when should Adjusted R¬≤ be used?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Both R¬≤ and Adjusted R¬≤ measure how well a regression model explains the variation in the dependent variable (Y).\n",
        "However, they differ in how they handle the number of independent variables in the model.\n",
        "\n",
        "1Ô∏è. R¬≤ (Coefficient of Determination):\n",
        "\n",
        "Measures the proportion of variance in Y explained by the independent variables.\n",
        "\n",
        "Formula:\n",
        "R¬≤ = 1 - (SSR / SST)\n",
        "\n",
        "Where:\n",
        "\n",
        "SSR = Sum of Squared Residuals\n",
        "\n",
        "SST = Total Sum of Squares\n",
        "\n",
        " Limitation:\n",
        "R¬≤ always increases when more variables are added ‚Äî even if they are irrelevant.\n",
        "This can make a model look better than it actually is.\n",
        "\n",
        "2Ô∏è. Adjusted R¬≤:\n",
        "\n",
        "Adjusted R¬≤ corrects R¬≤ by considering the number of predictors and the sample size.\n",
        "\n",
        "It increases only if a new variable improves the model more than would be expected by chance.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùê¥\n",
        "ùëë\n",
        "ùëó\n",
        "ùë¢\n",
        "ùë†\n",
        "ùë°\n",
        "ùëí\n",
        "ùëë\n",
        "\n",
        "ùëÖ\n",
        "2\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "[\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëÖ\n",
        "2\n",
        ")\n",
        "‚àó\n",
        "(\n",
        "ùëõ\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "(\n",
        "ùëõ\n",
        "‚àí\n",
        "ùëò\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "]\n",
        "Adjusted R\n",
        "2\n",
        "=1‚àí[(1‚àíR\n",
        "2\n",
        ")‚àó\n",
        "(n‚àík‚àí1)\n",
        "(n‚àí1)\n",
        "\t‚Äã\n",
        "\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "n = number of observations\n",
        "\n",
        "k = number of independent variables\n",
        "\n",
        "3Ô∏è. Key Difference Between R¬≤ and Adjusted R¬≤:\n",
        "Aspect\tR¬≤\tAdjusted R¬≤\n",
        "Definition\tMeasures how much variance in Y is explained by X\tAdjusts R¬≤ for the number of predictors\n",
        "Effect of Adding Variables\tAlways increases or stays same\tCan increase or decrease\n",
        "Penalization\tDoes not penalize extra variables\tPenalizes unnecessary variables\n",
        "Model Selection\tMisleading for multiple predictors\tBetter for comparing models\n",
        "\n",
        "4Ô∏è. Example:\n",
        "Model\tNumber of Predictors\tR¬≤\tAdjusted R¬≤\n",
        "Model 1\t2\t0.80\t0.78\n",
        "Model 2\t5\t0.82\t0.75\n",
        "\n",
        "Here, although R¬≤ increased from 0.80 to 0.82, Adjusted R¬≤ dropped from 0.78 to 0.75,\n",
        "showing that the extra variables did not improve the model meaningfully.\n",
        "\n",
        "5Ô∏è. When to Use Adjusted R¬≤:\n",
        "\n",
        "When comparing models with different numbers of independent variables.\n",
        "\n",
        "When you want a more accurate measure of how well the model generalizes to unseen data.\n",
        "\n",
        "Especially important in Multiple Linear Regression."
      ],
      "metadata": {
        "id": "nTimrOt7yzzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 21: What is the purpose of hypothesis testing in regression analysis?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The purpose of hypothesis testing in regression analysis is to determine whether the independent variables in the model have a statistically significant effect on the dependent variable (Y).\n",
        "It helps decide if the observed relationships in the data are real or just occurred by chance.\n",
        "\n",
        "1Ô∏è. Main Goal:\n",
        "\n",
        "To check whether each regression coefficient (slope) is significantly different from zero.\n",
        "\n",
        "If a coefficient ‚â† 0 ‚Üí that variable has a meaningful impact on Y.\n",
        "If a coefficient = 0 ‚Üí that variable does not significantly affect Y.\n",
        "\n",
        "2Ô∏è. Hypotheses in Regression:\n",
        "\n",
        "For each independent variable (X·µ¢):\n",
        "\n",
        "Null Hypothesis (H‚ÇÄ): Œ≤·µ¢ = 0\n",
        "‚Üí The variable X·µ¢ has no significant effect on Y.\n",
        "\n",
        "Alternative Hypothesis (H‚ÇÅ): Œ≤·µ¢ ‚â† 0\n",
        "‚Üí The variable X·µ¢ significantly affects Y.\n",
        "\n",
        "3Ô∏è. Test Used:\n",
        "\n",
        "Regression analysis uses the t-test for individual coefficients.\n",
        "\n",
        "t = (Estimated Coefficient ‚Äì 0) / Standard Error\n",
        "\n",
        "Then, compare the p-value with the significance level (Œ±) (usually 0.05):\n",
        "\n",
        "If p < 0.05 ‚Üí Reject H‚ÇÄ ‚Üí Variable is significant.\n",
        "\n",
        "If p ‚â• 0.05 ‚Üí Fail to reject H‚ÇÄ ‚Üí Variable is not significant.\n",
        "| Variable   | Coefficient | Std. Error | t-Value | p-Value | Interpretation  |\n",
        "| ---------- | ----------- | ---------- | ------- | ------- | --------------- |\n",
        "| Experience | 2.5         | 0.4        | 6.25    | 0.001   | Significant     |\n",
        "| Age        | 0.3         | 0.5        | 0.6     | 0.55    | Not significant |\n",
        "Here, ‚ÄúExperience‚Äù is statistically significant (p < 0.05),\n",
        "but ‚ÄúAge‚Äù is not, meaning Age doesn‚Äôt meaningfully affect the dependent variable.\n",
        "\n",
        "5Ô∏è. Other Hypothesis Tests in Regression:\n",
        "\n",
        "F-test: Tests the overall significance of the regression model (whether all coefficients together are significant).\n",
        "\n",
        "t-test: Tests the significance of individual coefficients.\n",
        "\n",
        "6Ô∏è. Why Hypothesis Testing Is Important:\n",
        "\n",
        "Identifies which variables meaningfully contribute to predictions.\n",
        "\n",
        "Helps refine models by removing non-significant predictors.\n",
        "\n",
        "Ensures the model is statistically valid and not based on random noise."
      ],
      "metadata": {
        "id": "y4D6BJifzR73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 22: What does a p-value represent in the context of regression analysis?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In regression analysis, the p-value measures the probability that the observed relationship between an independent variable (X) and the dependent variable (Y) occurred by random chance ‚Äî assuming that there is no real relationship in the population.\n",
        "\n",
        "It helps determine whether a regression coefficient is statistically significant.\n",
        "\n",
        "1Ô∏è. Definition:\n",
        "\n",
        "The p-value tests the null hypothesis (H‚ÇÄ) that a regression coefficient (Œ≤·µ¢) is equal to zero.\n",
        "\n",
        "H‚ÇÄ: Œ≤·µ¢ = 0 ‚Üí No relationship between X·µ¢ and Y\n",
        "\n",
        "H‚ÇÅ: Œ≤·µ¢ ‚â† 0 ‚Üí X·µ¢ significantly affects Y\n",
        "\n",
        "2. Decision rule\n",
        "| Condition    | Interpretation                                                        |\n",
        "| ------------ | --------------------------------------------------------------------- |\n",
        "| **p < 0.05** | Reject H‚ÇÄ ‚Üí Variable is **significant** (has real effect)             |\n",
        "| **p ‚â• 0.05** | Fail to reject H‚ÇÄ ‚Üí Variable is **not significant** (no clear effect) |\n",
        "\n",
        "3. Example\n",
        "| Variable   | Coefficient | p-value | Interpretation                   |\n",
        "| ---------- | ----------- | ------- | -------------------------------- |\n",
        "| Experience | 2.3         | 0.001   | Significant ‚Äî affects Y          |\n",
        "| Age        | 0.5         | 0.42    | Not significant ‚Äî no real effect |\n",
        "\n",
        " Meaning: There is only a 0.1% chance that Experience‚Äôs effect on Y is due to random noise ‚Äî hence, it‚Äôs statistically significant.\n",
        "\n",
        "4Ô∏è. How to Interpret p-value:\n",
        "\n",
        "Small p-value (close to 0): Strong evidence that X and Y are related.\n",
        "\n",
        "Large p-value (close to 1): Weak evidence; the observed effect may just be random.\n",
        "\n",
        "The smaller the p-value, the stronger the evidence against the null hypothesis.\n",
        "\n",
        "5Ô∏è. Importance in Regression:\n",
        "\n",
        "Determines which predictors meaningfully influence the dependent variable.\n",
        "\n",
        "Helps in feature selection ‚Äî keeping only statistically significant variables.\n",
        "\n",
        "Ensures the model‚Äôs results are reliable and valid."
      ],
      "metadata": {
        "id": "4AwORaGbzvlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 23: Explain the difference between Type I and Type II errors in hypothesis testing.\n",
        "\n",
        "Answer:\n",
        "\n",
        "In hypothesis testing, two types of errors can occur when making decisions about the null hypothesis (H‚ÇÄ).\n",
        "These are called Type I and Type II errors.\n",
        "They represent the risks of making incorrect conclusions based on sample data.\n",
        "\n",
        "1Ô∏è. Type I Error (False Positive):\n",
        "\n",
        "Occurs when the null hypothesis (H‚ÇÄ) is true, but we wrongly reject it.\n",
        "\n",
        "In other words, we detect an effect that does not actually exist.\n",
        "\n",
        "Represented by Œ± (alpha) ‚Äî the significance level of the test (commonly 0.05 or 5%).\n",
        "\n",
        "Example:\n",
        "You conclude that a medicine works (reject H‚ÇÄ),\n",
        "but in reality, it has no real effect (H‚ÇÄ is true).\n",
        " Think of it as: ‚ÄúCrying wolf when there‚Äôs no wolf.‚Äù\n",
        "\n",
        "2Ô∏è. Type II Error (False Negative):\n",
        "\n",
        "Occurs when the null hypothesis (H‚ÇÄ) is false, but we fail to reject it.\n",
        "\n",
        "In other words, we miss detecting a real effect.\n",
        "\n",
        "Represented by Œ≤ (beta).\n",
        "\n",
        "Example:\n",
        "You conclude that a medicine does not work (fail to reject H‚ÇÄ),\n",
        "but in reality, it does work (H‚ÇÄ is false).\n",
        "\n",
        " Think of it as: ‚ÄúMissing the wolf when it‚Äôs really there.‚Äù\n",
        " 3Ô∏è. Summary Table:\n",
        "\n",
        " | Type of Error   | What Happens              | Truth       | Decision             | Consequence    |\n",
        "| --------------- | ------------------------- | ----------- | -------------------- | -------------- |\n",
        "| **Type I (Œ±)**  | Reject a true H‚ÇÄ          | H‚ÇÄ is true  | Wrongly reject H‚ÇÄ    | False positive |\n",
        "| **Type II (Œ≤)** | Fail to reject a false H‚ÇÄ | H‚ÇÄ is false | Miss the true effect | False negative |\n",
        "\n",
        "4Ô∏è. Real-World Example (Medical Test):\n",
        "| Situation                          | Reality        | Test Result       | Type of Error |\n",
        "| ---------------------------------- | -------------- | ----------------- | ------------- |\n",
        "| Patient is healthy, test says sick | False positive | **Type I Error**  |               |\n",
        "| Patient is sick, test says healthy | False negative | **Type II Error** |               |\n",
        "5Ô∏è. Relationship Between Œ± and Œ≤:\n",
        "\n",
        "Lowering Œ± (reducing false positives) usually increases Œ≤ (more false negatives).\n",
        "\n",
        "There‚Äôs always a trade-off between the two errors.\n",
        "\n",
        "The power of a test = (1 ‚Äì Œ≤) ‚Üí Probability of correctly detecting a real effect.\n",
        "\n",
        "6Ô∏è. Key Takeaways:\n",
        "\n",
        "Type I error: Rejecting a true null hypothesis ‚Üí detecting a false effect.\n",
        "\n",
        "Type II error: Failing to reject a false null hypothesis ‚Üí missing a real effect.\n",
        "\n",
        "Both errors can lead to wrong conclusions, so they must be minimized through good experimental design and sample size selection."
      ],
      "metadata": {
        "id": "uTUA01Fl0WVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 24: What does the F-test evaluate in Multiple Linear Regression?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In Multiple Linear Regression, the F-test is used to determine whether the overall regression model is statistically significant ‚Äî that is, whether the independent variables collectively have a meaningful impact on the dependent variable (Y).\n",
        "\n",
        "It tests the joint significance of all regression coefficients (except the intercept).\n",
        "\n",
        "1Ô∏è. Purpose of the F-test:\n",
        "\n",
        "To test whether at least one independent variable in the model has a non-zero coefficient, meaning it has a significant effect on Y.\n",
        "\n",
        "2Ô∏è. Hypotheses:\n",
        "\n",
        "Null Hypothesis (H‚ÇÄ):\n",
        "Œ≤‚ÇÅ = Œ≤‚ÇÇ = Œ≤‚ÇÉ = ... = Œ≤‚Çñ = 0\n",
        "‚Üí None of the independent variables affect Y.\n",
        "\n",
        "Alternative Hypothesis (H‚ÇÅ):\n",
        "At least one Œ≤·µ¢ ‚â† 0\n",
        "‚Üí At least one independent variable significantly affects Y.\n",
        "\n",
        "3Ô∏è. F-test Formula:\n",
        "\n",
        "ùêπ\n",
        "=\n",
        "(\n",
        "ùëÖ\n",
        "2\n",
        "/\n",
        "ùëò\n",
        ")\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëÖ\n",
        "2\n",
        ")\n",
        "/\n",
        "(\n",
        "ùëõ\n",
        "‚àí\n",
        "ùëò\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "F=\n",
        "(1‚àíR\n",
        "2\n",
        ")/(n‚àík‚àí1)\n",
        "(R\n",
        "2\n",
        "/k)\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "R¬≤ = Coefficient of determination\n",
        "\n",
        "k = Number of independent variables\n",
        "\n",
        "n = Number of observations\n",
        "\n",
        "4Ô∏è. Decision Rule:\n",
        "\n",
        "Compare the calculated F-value with the critical F-value from the F-distribution table (based on degrees of freedom).\n",
        "\n",
        "Or use the p-value:\n",
        "| Condition    | Interpretation                                 |\n",
        "| ------------ | ---------------------------------------------- |\n",
        "| **p < 0.05** | Reject H‚ÇÄ ‚Üí Model is statistically significant |\n",
        "| **p ‚â• 0.05** | Fail to reject H‚ÇÄ ‚Üí Model is not significant   |\n",
        "5Ô∏è. Example:\n",
        "\n",
        "Suppose you are predicting house price (Y) using 3 predictors: size, bedrooms, and location.\n",
        "If the F-test p-value = 0.001,\n",
        "‚Üí Reject H‚ÇÄ ‚Üí At least one predictor significantly explains variation in house price.\n",
        "This means the overall model fits the data well.\n",
        "6Ô∏è. Difference from t-test:\n",
        "| Test       | Purpose                                         |\n",
        "| ---------- | ----------------------------------------------- |\n",
        "| **t-test** | Checks significance of **individual** variables |\n",
        "| **F-test** | Checks significance of the **entire model**     |\n",
        "7Ô∏è. Importance of the F-test:\n",
        "\n",
        "Confirms that the regression model provides a better fit than a model with no predictors.\n",
        "\n",
        "Used as an initial check before interpreting individual coefficients.\n",
        "\n",
        "Ensures that the model as a whole has predictive power."
      ],
      "metadata": {
        "id": "hFEFVrzx1CGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 25: What are residuals in regression, and why are they important?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In regression analysis, residuals are the differences between the actual values of the dependent variable (Y) and the predicted values (≈∂) from the regression model.\n",
        "Residuals represent the error or unexplained part of the model.\n",
        "\n",
        "1Ô∏è. Formula for Residuals:\n",
        "\n",
        "ùëÖ\n",
        "ùëí\n",
        "ùë†\n",
        "ùëñ\n",
        "ùëë\n",
        "ùë¢\n",
        "ùëé\n",
        "ùëô\n",
        "(\n",
        "ùëí\n",
        "ùëñ\n",
        ")\n",
        "=\n",
        "ùê¥\n",
        "ùëê\n",
        "ùë°\n",
        "ùë¢\n",
        "ùëé\n",
        "ùëô\n",
        "\n",
        "ùëâ\n",
        "ùëé\n",
        "ùëô\n",
        "ùë¢\n",
        "ùëí\n",
        "(\n",
        "ùëå\n",
        "ùëñ\n",
        ")\n",
        "‚àí\n",
        "ùëÉ\n",
        "ùëü\n",
        "ùëí\n",
        "ùëë\n",
        "ùëñ\n",
        "ùëê\n",
        "ùë°\n",
        "ùëí\n",
        "ùëë\n",
        "\n",
        "ùëâ\n",
        "ùëé\n",
        "ùëô\n",
        "ùë¢\n",
        "ùëí\n",
        "(\n",
        "ùëå\n",
        "^\n",
        "ùëñ\n",
        ")\n",
        "Residual(e\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")=Actual Value(Y\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")‚àíPredicted Value(\n",
        "Y\n",
        "^\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "So each data point has its own residual ‚Äî showing how far the model‚Äôs prediction is from the true observation.\n",
        "\n",
        "2Ô∏è. Example:\n",
        "| Actual Y (Y·µ¢) | Predicted Y (≈∂·µ¢) | Residual (e·µ¢ = Y·µ¢ - ≈∂·µ¢) |\n",
        "| ------------- | ---------------- | ----------------------- |\n",
        "| 10            | 9                | +1                      |\n",
        "| 8             | 7                | +1                      |\n",
        "| 5             | 6                | -1                      |\n",
        " Interpretation:\n",
        "A positive residual means the model underpredicted (prediction too low).\n",
        "A negative residual means the model overpredicted (prediction too high).\n",
        "\n",
        "3Ô∏è. Importance of Residuals:\n",
        "\n",
        "Residuals help evaluate the accuracy and reliability of a regression model.\n",
        "\n",
        "a) Check Model Fit:\n",
        "\n",
        "If residuals are small and randomly distributed, the model fits well.\n",
        "\n",
        "If residuals show patterns, the model might be missing important variables or have incorrect form.\n",
        "\n",
        "b) Check Linearity:\n",
        "\n",
        "Residual plots help verify if the relationship between X and Y is truly linear.\n",
        "\n",
        "c) Detect Outliers:\n",
        "\n",
        "Large residuals indicate data points that deviate significantly from model predictions.\n",
        "\n",
        "d) Check Homoscedasticity:\n",
        "\n",
        "Residuals should have constant variance across all fitted values.\n",
        "\n",
        "Changing variance (heteroscedasticity) indicates model issues.\n",
        "\n",
        "4Ô∏è. Residual Plot Example:\n",
        "\n",
        "When plotting residuals (y-axis) vs. predicted values (x-axis):\n",
        "| Pattern        | Interpretation                             |\n",
        "| -------------- | ------------------------------------------ |\n",
        "| Random scatter | Good model fit                             |\n",
        "| Funnel shape   | Heteroscedasticity (non-constant variance) |\n",
        "| Curved pattern | Non-linear relationship                    |\n",
        "5Ô∏è. Residual Sum of Squares (RSS):\n",
        "\n",
        "To measure the total error:\n",
        "\n",
        "ùëÖ\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "=\n",
        "Œ£\n",
        "(\n",
        "ùëå\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëå\n",
        "^\n",
        "ùëñ\n",
        ")\n",
        "2\n",
        "RSS=Œ£(Y\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚àí\n",
        "Y\n",
        "^\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "2\n",
        "\n",
        "A smaller RSS means better model accuracy.\n",
        "\n",
        "6Ô∏è. Key Uses of Residuals:\n",
        "\n",
        "Assess model assumptions.\n",
        "\n",
        "Detect problems like outliers or non-linearity.\n",
        "\n",
        "Improve the model by identifying missing features.\n",
        "\n",
        "Evaluate prediction performance."
      ],
      "metadata": {
        "id": "9JXXNvT71oGv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhz_aLZ9lIBw"
      },
      "outputs": [],
      "source": []
    }
  ]
}